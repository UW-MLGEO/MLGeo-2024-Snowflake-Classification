{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Architecture Exploration and Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the `data_EDA_and_CML_benchmarking.ipynb` notebook for parts 1 and 2, which include the deep learning dataset preparation and CML benchmarking, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Architecture Exploration: Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall, the performances of the initial four deep learning models implemented in the `data_EDA_and_CML_benchmarking.ipynb` notebook, which included FCN, CNN, ResNet, and RNN, were poor. Among them, the CNN had the highest accuracy, exceeding 25%. While this value is still low, we will focus on implementing architectures that utilize CNNs, focusing on the three architectures listed below:  \n",
    "\n",
    "1. VGG16 with Fine-Tuning (a deep CNN)\n",
    "* *Why?* A VGG16 is a deep CNN with 16 layers that excels at deep feature extraction, effectively capturing complex visual features through small 3x3 convolutional filters. By using pre-trained weights on ImageNet and fine-tuning them on the `PHIPS_CrystalHabitAI_Dataset.nc` image dataset, VGG16 can adapt to our specific classification task, improving performance even with limited data, as the `PHIPS_CrystalHabitAI_Dataset.nc` image dataset is relatively small. The VGG16's depth and fine-tuning capabilities help overcome the low accuracy of initial models by learning more intricate patterns specific to our ice crystal images.\n",
    "\n",
    "2. InceptionV3 (a different variation of a deep CNN)\n",
    "* *Why?* This architecture excels at multi-scale feature learning, utilizing Inception modules to process multiple convolutional filter sizes in parallel, capturing visual information at different scales within the same layer. Despite its depth, InceptionV3 is computationally efficient due to techniques like factorized convolutions and dimension reductions, making it suitable for complex datasets without excessive computational cost. Its advanced architecture can extract richer and more diverse features than simpler models, potentially leading to significant improvements in classification accuracy on the `PHIPS_CrystalHabitAI_Dataset.nc` image dataset.\n",
    "\n",
    "3. Convolutional Recurrent Neural Network (CRNN) with Attention Mechanism (a hyrbid of CNN and RNN)\n",
    "* *Why?* CRNN integrates Convolutional Neural Networks for spatial feature extraction with Recurrent Neural Networks (like LSTM or GRU) to capture sequential or temporal dependencies in the data. Incorporating attention layers enables the model to focus on the most relevant parts of the input images, enhancing its ability to learn important features and improving classification results. Lastly, this architecture offers a novel solution that goes beyond standard models, potentially capturing complex patterns and relationships in our ice crystal images that previous models may have missed.\n",
    "\n",
    "#### By using these DL architectures, we will address the low performance of the initial DL models by leveraging deeper networks, advanced feature extraction techniques, and innovative combinations of neural network types tailored to our image classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.2 Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import time \n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 11:35:32.148225: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-03 11:35:32.151913: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-03 11:35:32.162824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733254532.181056   24546 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733254532.186547   24546 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 11:35:32.203733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv2D, MaxPooling2D, \n",
    "                                     GlobalAveragePooling2D, Input, SimpleRNN, LSTM, TimeDistributed, \n",
    "                                     Bidirectional, Attention)\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.preprocessing.image import smart_resize\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Concatenate, Resizing, Reshape, Permute, Multiply, Activation, RepeatVector, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn for metrics\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                             f1_score, precision_score, recall_score, mean_squared_error, roc_curve, auc)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Loading and Preprocessing\n",
    "##### organized using a `DatasetLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the dataset using xarray\n",
    "        ds = xr.open_dataset(self.file_path)\n",
    "        images = ds['image_array'].values  # Shape: (samples, height, width)\n",
    "        labels = ds['label'].values        # Shape: (samples,)\n",
    "        temps = ds['temperature'].values   # Shape: (samples,)\n",
    "        return images, labels, temps\n",
    "\n",
    "    def preprocess_data(self, images, labels):\n",
    "        # Encode string labels into integers\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)\n",
    "        num_classes = len(np.unique(labels_encoded))\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        labels_one_hot = to_categorical(labels_encoded, num_classes)\n",
    "\n",
    "        # Expand dimensions of images for channels (grayscale images)\n",
    "        images_expanded = np.expand_dims(images, axis=-1)  # Shape: (samples, height, width, 1)\n",
    "\n",
    "        # Normalize images to [0, 1]\n",
    "        images_normalized = images_expanded / 255.0\n",
    "\n",
    "        return images_normalized, labels_one_hot, labels_encoded, num_classes, label_encoder\n",
    "\n",
    "    def split_data(self, images, labels_encoded, labels_one_hot, temps):\n",
    "        # First split: training set and temp set\n",
    "        X_train, X_temp, y_train_encoded, y_temp_encoded, y_train_one_hot, y_temp_one_hot, temp_train, temp_temp = train_test_split(\n",
    "            images, labels_encoded, labels_one_hot, temps, test_size=0.2, random_state=42, stratify=labels_encoded)\n",
    "\n",
    "        # Second split: validation set and test set\n",
    "        X_val, X_test, y_val_encoded, y_test_encoded, y_val_one_hot, y_test_one_hot, temp_val, temp_test = train_test_split(\n",
    "            X_temp, y_temp_encoded, y_temp_one_hot, temp_temp, test_size=0.5, random_state=42, stratify=y_temp_encoded)\n",
    "\n",
    "        return (X_train, y_train_encoded, y_train_one_hot, temp_train), \\\n",
    "               (X_val, y_val_encoded, y_val_one_hot, temp_val), \\\n",
    "               (X_test, y_test_encoded, y_test_one_hot, temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DatasetLoader and load the data\n",
    "# data_loader = DatasetLoader('/Users/valeriagarcia/Desktop/ESS569_Snowflake_Classification/PHIPS_CrystalHabitAI_Dataset.nc')\n",
    "data_loader = DatasetLoader('/home/disk/meso-home/vgarcia1/PHIPS_classification/PHIPS_CrystalHabitAI_Dataset.nc')\n",
    "images, labels, temps = data_loader.load_data()\n",
    "images, labels_one_hot, labels_encoded, num_classes, label_encoder = data_loader.preprocess_data(images, labels)\n",
    "\n",
    "# Split data including temperatures\n",
    "(X_train, y_train_encoded, y_train_one_hot, temp_train), \\\n",
    "(X_val, y_val_encoded, y_val_one_hot, temp_val), \\\n",
    "(X_test, y_test_encoded, y_test_one_hot, temp_test) = data_loader.split_data(images, labels_encoded, labels_one_hot, temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (352, 1024, 1360, 1)\n",
      "y_train_one_hot shape: (352, 11)\n",
      "temp_train shape: (352,)\n",
      "Data type of temp_train: float64\n"
     ]
    }
   ],
   "source": [
    "# Check Shapes and Data Types\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train_one_hot shape:\", y_train_one_hot.shape)\n",
    "print(\"temp_train shape:\", temp_train.shape)\n",
    "print(\"Data type of temp_train:\", temp_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Data Augmentation\n",
    "##### Here, we create a data augmentation generator (`data_generator`) for the training data that applies random transformations—including rotations up to 20 degrees, horizontal and vertical shifts up to 10% of the image size, horizontal and vertical flips, zooms up to 10%—to enhance the diversity of the dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, labels_one_hot, temperatures, batch_size, augment=False):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0,\n",
    "        rotation_range=20 if augment else 0,\n",
    "        width_shift_range=0.1 if augment else 0,\n",
    "        height_shift_range=0.1 if augment else 0,\n",
    "        horizontal_flip=augment,\n",
    "        vertical_flip=augment,\n",
    "        zoom_range=0.1 if augment else 0\n",
    "    )\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels_one_hot = np.array(labels_one_hot)\n",
    "    temperatures = np.array(temperatures)\n",
    "    num_samples = images.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        if augment:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            x_batch = images[batch_indices]\n",
    "            y_batch = labels_one_hot[batch_indices]\n",
    "            temp_batch = temperatures[batch_indices]\n",
    "            \n",
    "            # Resize images using the original method\n",
    "            x_batch_resized = np.empty((x_batch.shape[0], *target_size, x_batch.shape[-1]))\n",
    "            for i, img in enumerate(x_batch):\n",
    "                x_batch_resized[i] = tf.image.resize(img, target_size).numpy()\n",
    "            \n",
    "            # Apply data augmentation\n",
    "            x_batch_augmented = np.empty_like(x_batch_resized)\n",
    "            for i, img in enumerate(x_batch_resized):\n",
    "                x_batch_augmented[i] = datagen.random_transform(img)\n",
    "            \n",
    "            yield (x_batch_augmented, temp_batch), y_batch\n",
    "\n",
    "# Create data generators\n",
    "batch_size = 16\n",
    "target_size = (256, 256) \n",
    "\n",
    "train_generator = data_generator(\n",
    "    X_train, y_train_one_hot, temp_train, batch_size, augment=True\n",
    ")\n",
    "val_generator = data_generator(\n",
    "    X_val, y_val_one_hot, temp_val, batch_size, augment=False\n",
    ")\n",
    "test_generator = data_generator(\n",
    "    X_test, y_test_one_hot, temp_test, batch_size, augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch shape: (16, 256, 256, 1)\n",
      "Temperature batch shape: (16,)\n",
      "Label batch shape: (16, 11)\n",
      "Data generator outputs verified successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 11:35:51.524218: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Check if the data generators produce batches with the expected shapes, types, and values\n",
    "\n",
    "# Fetch a batch from the train generator\n",
    "(data_batch, temp_batch), label_batch = next(train_generator)\n",
    "\n",
    "# Compute expected shapes dynamically\n",
    "expected_data_shape = (batch_size, *target_size, 1)  # Use target_size for dynamic shape calculation\n",
    "expected_temp_shape = (batch_size,)  # Temperature batch shape\n",
    "expected_label_shape = (batch_size, y_train_one_hot.shape[1])  # Match number of classes dynamically\n",
    "\n",
    "# Verify resized shapes\n",
    "assert data_batch.shape == expected_data_shape, f\"Data batch shape mismatch: {data_batch.shape}, expected: {expected_data_shape}\"\n",
    "assert temp_batch.shape == expected_temp_shape, f\"Temperature batch shape mismatch: {temp_batch.shape}, expected: {expected_temp_shape}\"\n",
    "assert label_batch.shape == expected_label_shape, f\"Label batch shape mismatch: {label_batch.shape}, expected: {expected_label_shape}\"\n",
    "\n",
    "# Print the verified shapes for debugging\n",
    "print(f\"Data batch shape: {data_batch.shape}\")\n",
    "print(f\"Temperature batch shape: {temp_batch.shape}\")\n",
    "print(f\"Label batch shape: {label_batch.shape}\")\n",
    "\n",
    "print(\"Data generator outputs verified successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Physics-Informed Loss Function with Probabilistic Class Likelihoods\n",
    "\n",
    "##### In the cloud microphysics community, it is well-understood from laboratory studies that different ice crystal habits have a tendency to grow within a specific range of temperatures and relative humidity conditions. An example of the different temperature regimes is provided in Varcie et al. 2024:\n",
    "* *polycrystalline growth layer* (growth of polycrstals) -  may occur when the ambient temperature is below -18˚C\n",
    "* *dendritic growth layer* (growth of dendrites) - may occur when temperature is warmer than or equal to -18˚C and less than or equal to -12˚C\n",
    "* *plate growth layer* (growth of plates) - may occur where temperature is warmer than -12˚C and less than -8˚C\n",
    "* *needle growth layer* (growth of needles) - may occur where temperatures are warmer than or equal to -8˚C and less than -3˚C\n",
    "\n",
    "##### As our dataset only contains temperature in the metadata, we will focus on leveraging temperature and the temperature regimes above to create a custom loss function. Note the above temperature ranges refer to temperature layers over which certain ice crystals *may* grow, assuming other conditions, such as high ice/water supersaturations, are met. Moreover, particles that growth at cooler temperatures may still be observed at warmer temperatures due to sedimentation of the particles. \n",
    "\n",
    "##### Objective: Incorporate temperature-dependent class probabilities into the loss function to guide the model based on physical principles while allowing for natural variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24546/2751809565.py:24: RuntimeWarning: Mean of empty slice\n",
      "  mean_temp = np.nanmean(temps_)\n",
      "/home/disk/meso-home/vgarcia1/.local/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1872: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "#### Create a mapping from class labels to their corresponding temperatures ####\n",
    "\n",
    "# Get unique class labels\n",
    "unique_classes = np.unique(labels_encoded)\n",
    "\n",
    "# Initialize a dictionary to hold temperatures for each class\n",
    "class_temperatures = {class_idx: [] for class_idx in unique_classes}\n",
    "\n",
    "# Populate the dictionary\n",
    "for idx, class_idx in enumerate(labels_encoded):\n",
    "    temp = temps[idx]\n",
    "    class_temperatures[class_idx].append(temp)\n",
    "\n",
    "#### Calculate the mean and standard deviation for the temperatures in each class ####\n",
    "\n",
    "# Initialize the dictionary to hold temperature statistics for each class\n",
    "class_temperature_stats = {}\n",
    "\n",
    "# Compute mean and standard deviation for each class, ignoring NaNs\n",
    "for class_idx in unique_classes:\n",
    "    temps_ = np.array(class_temperatures[class_idx])\n",
    "    \n",
    "    # Compute mean and standard deviation while ignoring NaNs\n",
    "    mean_temp = np.nanmean(temps_)\n",
    "    std_temp = np.nanstd(temps_)\n",
    "    \n",
    "    # Handle case where all temps are NaN\n",
    "    if np.isnan(mean_temp) or np.isnan(std_temp):\n",
    "        class_temperature_stats[class_idx] = {'mean': np.nan, 'std': np.nan}\n",
    "    else:\n",
    "        class_temperature_stats[class_idx] = {'mean': mean_temp, 'std': std_temp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected num_classes: 11, Actual num_classes: 11\n",
      "Number of classes matches the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Confirm the number of unique classes\n",
    "actual_num_classes = len(np.unique(labels_encoded))\n",
    "print(f\"Expected num_classes: {num_classes}, Actual num_classes: {actual_num_classes}\")\n",
    "\n",
    "# Ensure consistency\n",
    "if actual_num_classes != num_classes:\n",
    "    raise ValueError(\"Mismatch between num_classes and the actual number of classes in the dataset!\")\n",
    "else:\n",
    "    print(\"Number of classes matches the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mappings (Integer to Original Label):\n",
      "Class 0: aggregate\n",
      "Class 1: bullet_rosette\n",
      "Class 2: capped_column\n",
      "Class 3: column\n",
      "Class 4: dendrite\n",
      "Class 5: graupel\n",
      "Class 6: needle\n",
      "Class 7: plate\n",
      "Class 8: polycrystal\n",
      "Class 9: side_plane\n",
      "Class 10: tiny\n",
      "\n",
      "Class Distribution in Dataset:\n",
      "Class 0: 40 samples\n",
      "Class 1: 40 samples\n",
      "Class 2: 40 samples\n",
      "Class 3: 40 samples\n",
      "Class 4: 40 samples\n",
      "Class 5: 40 samples\n",
      "Class 6: 40 samples\n",
      "Class 7: 40 samples\n",
      "Class 8: 40 samples\n",
      "Class 9: 40 samples\n",
      "Class 10: 40 samples\n"
     ]
    }
   ],
   "source": [
    "# Print the mapping of integer-encoded labels to original class labels\n",
    "print(\"Class Mappings (Integer to Original Label):\")\n",
    "for class_idx, class_label in enumerate(label_encoder.classes_):\n",
    "    print(f\"Class {class_idx}: {class_label}\")\n",
    "\n",
    "# Check the number of samples per class in the dataset\n",
    "print(\"\\nClass Distribution in Dataset:\")\n",
    "unique_classes, class_counts = np.unique(labels_encoded, return_counts=True)\n",
    "for class_idx, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Class {class_idx}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Mean Temp = -11.28, Std Temp = 4.97\n",
      "Class 1: Temperature stats are missing (mean/std are NaN).\n",
      "Class 2: Mean Temp = -11.40, Std Temp = 2.89\n",
      "Class 3: Mean Temp = -13.08, Std Temp = 4.77\n",
      "Class 4: Mean Temp = -14.17, Std Temp = 2.99\n",
      "Class 5: Mean Temp = -11.17, Std Temp = 4.74\n",
      "Class 6: Mean Temp = -3.14, Std Temp = 1.18\n",
      "Class 7: Mean Temp = -9.33, Std Temp = 4.09\n",
      "Class 8: Mean Temp = -13.55, Std Temp = 4.28\n",
      "Class 9: Mean Temp = -9.21, Std Temp = 4.76\n",
      "Class 10: Mean Temp = -8.36, Std Temp = 3.61\n",
      "Temperature values for Class 0: [-15.21, -15.28, -15.25, -15.23, -15.23, -15.29, -15.34, -15.3, -15.28, -15.11, -15.01, -14.8, -12.16, -15.15, -15.0, -14.85, nan, -15.25, -17.89, -16.53, -16.33, -16.61, -7.67, -5.09, -4.64, -4.32, nan, nan, nan, -5.64, -5.77, -5.69, -5.23, -5.09, -4.9, -4.82, -4.89, -4.72, -4.79, -10.6]\n"
     ]
    }
   ],
   "source": [
    "# Verify class temperature statistics\n",
    "for class_idx, stats in class_temperature_stats.items():\n",
    "    if np.isnan(stats['mean']) or np.isnan(stats['std']):\n",
    "        print(f\"Class {class_idx}: Temperature stats are missing (mean/std are NaN).\")\n",
    "    else:\n",
    "        print(f\"Class {class_idx}: Mean Temp = {stats['mean']:.2f}, Std Temp = {stats['std']:.2f}\")\n",
    "\n",
    "# Spot-check the temperature values for a specific class\n",
    "class_to_check = 0  # Replace with a class index to check\n",
    "print(f\"Temperature values for Class {class_to_check}: {class_temperatures[class_to_check]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN temperatures: 111/440 (25.23%)\n",
      "Class 0 has 4 NaN temperature(s).\n",
      "Class 1 has 40 NaN temperature(s).\n",
      "Class 2 has 6 NaN temperature(s).\n",
      "Class 3 has 12 NaN temperature(s).\n",
      "Class 5 has 1 NaN temperature(s).\n",
      "Class 6 has 18 NaN temperature(s).\n",
      "Class 7 has 6 NaN temperature(s).\n",
      "Class 9 has 17 NaN temperature(s).\n",
      "Class 10 has 7 NaN temperature(s).\n"
     ]
    }
   ],
   "source": [
    "# Check the percentage of NaN temperatures\n",
    "nan_count = np.isnan(temps).sum()\n",
    "total_samples = len(temps)\n",
    "print(f\"Number of NaN temperatures: {nan_count}/{total_samples} ({(nan_count/total_samples)*100:.2f}%)\")\n",
    "\n",
    "# Investigate classes with NaN temperatures\n",
    "for class_idx, temps_ in class_temperatures.items():\n",
    "    nan_temps = [temp for temp in temps_ if np.isnan(temp)]\n",
    "    if nan_temps:\n",
    "        print(f\"Class {class_idx} has {len(nan_temps)} NaN temperature(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The temperature statistics are all NaN for the bullet rosette group (Class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_expected_probs() function calculates the expected class probabilities for each sample in a batch based on its temperature, using Gaussian distributions derived from the class temperature statistics.\n",
    "# This function will be called within the physics-informed loss function to obtain the expected probabilities based on temperature, which are then used to compute the physics term (e.g., KL divergence).\n",
    "# Output: Normalized probabilities for each sample in a batch.\n",
    "\n",
    "def get_expected_probs(temperature_batch, num_classes):\n",
    "    # Initialize expected probabilities array\n",
    "    expected_probs = np.zeros((len(temperature_batch), num_classes), dtype=np.float32)\n",
    "    \n",
    "    for i, temp in enumerate(temperature_batch):\n",
    "        total_prob = 0.0\n",
    "        probs = np.zeros(num_classes, dtype=np.float32)\n",
    "        \n",
    "        # Handle NaN temperatures by assigning uniform probabilities\n",
    "        if np.isnan(temp):\n",
    "            # Assign uniform probability if temperature is NaN\n",
    "            probs[:] = 1.0 / num_classes\n",
    "        else:\n",
    "            for class_idx in range(num_classes):\n",
    "                mean = class_temperature_stats[class_idx]['mean']\n",
    "                std = class_temperature_stats[class_idx]['std']\n",
    "                \n",
    "                # Handle classes with NaN mean or std by assigning uniform probability\n",
    "                if np.isnan(mean) or np.isnan(std):\n",
    "                    prob = 1.0 / num_classes\n",
    "                else:\n",
    "                    # Gaussian probability density function\n",
    "                    prob = np.exp(-0.5 * ((temp - mean) / std) ** 2) / (std * np.sqrt(2 * np.pi))\n",
    "                probs[class_idx] = prob\n",
    "                total_prob += prob\n",
    "            \n",
    "            # Normalize probabilities to sum to 1\n",
    "            if total_prob > 0:\n",
    "                probs /= total_prob\n",
    "            else:\n",
    "                # If total_prob is zero (unlikely), assign uniform probabilities\n",
    "                probs[:] = 1.0 / num_classes\n",
    "        \n",
    "        expected_probs[i] = probs\n",
    "    \n",
    "    return expected_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the categorical cross-entropy loss with a physics-informed KL divergence term.\n",
    "# This physics-informed loss function will be modified to exclude classes with missing temperature data (e.g., Class 1) from the physics term.\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "def physics_informed_loss(y_true, y_pred, temperature):\n",
    "    # Ensure all inputs are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    temperature = tf.cast(temperature, tf.float32)\n",
    "\n",
    "    # Standard categorical cross-entropy loss\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    loss = cce(y_true, y_pred)\n",
    "\n",
    "    # Identify samples not belonging to classes with missing temperature stats\n",
    "    class_indices = tf.argmax(y_true, axis=1)\n",
    "    valid_class_mask = tf.constant([\n",
    "        not np.isnan(class_temperature_stats[i]['mean']) for i in range(num_classes)\n",
    "    ], dtype=tf.bool)\n",
    "\n",
    "    sample_mask = tf.gather(valid_class_mask, class_indices)\n",
    "    sample_mask = tf.cast(sample_mask, tf.float32)\n",
    "\n",
    "    # Compute expected probabilities based on temperature\n",
    "    expected_probs_np = tf.numpy_function(\n",
    "        func=get_expected_probs,\n",
    "        inp=[temperature, num_classes],\n",
    "        Tout=tf.float32\n",
    "    )\n",
    "\n",
    "    # Ensure the shape of expected_probs is defined\n",
    "    expected_probs = tf.ensure_shape(expected_probs_np, [None, num_classes])  # [batch_size, num_classes]\n",
    "\n",
    "    # Compute the physics term (KL divergence)\n",
    "    kl_divergence = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    physics_term = kl_divergence(expected_probs, y_pred)\n",
    "\n",
    "    # Apply the mask to exclude invalid samples\n",
    "    physics_term = physics_term * sample_mask\n",
    "\n",
    "    # Compute mean physics term over valid samples\n",
    "    total_valid_samples = tf.reduce_sum(sample_mask) + 1e-7  # Avoid division by zero\n",
    "    physics_term = tf.reduce_sum(physics_term) / total_valid_samples\n",
    "\n",
    "    # Total loss with weighting factor\n",
    "    lambda_weight = 0.1  # Adjust as needed\n",
    "    total_loss = loss + lambda_weight * physics_term\n",
    "\n",
    "    # Ensure the total loss is a scalar\n",
    "    total_loss = tf.reduce_mean(total_loss)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the loss function with dummy data to ensure it works without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.4944966\n"
     ]
    }
   ],
   "source": [
    "# Set your actual number of classes\n",
    "num_classes = 11\n",
    "\n",
    "# Dummy data for testing\n",
    "y_true_test = tf.one_hot([0, 1], depth=num_classes)\n",
    "y_pred_test = tf.constant([[0.1]*num_classes, [0.1]*num_classes], dtype=tf.float32)\n",
    "temp_test = tf.constant([0.0, -5.0], dtype=tf.float32)\n",
    "\n",
    "# Call the loss function\n",
    "test_loss = physics_informed_loss(y_true_test, y_pred_test, temp_test)\n",
    "print(\"Test loss:\", test_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The loss value obtained is reasonable and aligns with the expected calculations given the dummy data. The loss value is relatively high (~2.49) because the predicted probabilities are uniform and do not match the true labels. We have set the predicted probabilities to be uniform across all classes, so predicting the same probability for all classes (especially when incorrect) results in a higher loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Loss Value: 2.6170204\n"
     ]
    }
   ],
   "source": [
    "# Verifying Temperature-Based Loss Function one more time to ensure the function works correctly with model outputs and temperature inputs, using a small batch of dummy data to simulate training and testing scenarios\n",
    "\n",
    "# Dummy data for verification\n",
    "dummy_images = np.random.rand(2, 1024, 1360, 1).astype(np.float32)  # 2 grayscale images\n",
    "dummy_labels = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])  # One-hot labels\n",
    "dummy_temps = np.array([0.0, -5.0], dtype=np.float32)  # 2 temperature values\n",
    "\n",
    "# Simulate model predictions (random probabilities)\n",
    "dummy_predictions = np.random.rand(2, 11).astype(np.float32)\n",
    "dummy_predictions /= np.sum(dummy_predictions, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "\n",
    "# Compute the loss\n",
    "dummy_loss = physics_informed_loss(\n",
    "    tf.constant(dummy_labels, dtype=tf.float32),\n",
    "    tf.constant(dummy_predictions, dtype=tf.float32),\n",
    "    tf.constant(dummy_temps, dtype=tf.float32)\n",
    ")\n",
    "\n",
    "print(\"Dummy Loss Value:\", dummy_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lastly, to handle the passing of temperature data to the loss function, we'll use a subclassed model with a custom `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        try:\n",
    "            # Adjust unpacking to handle nested structure\n",
    "            if len(data) == 3:  # If there are extra elements (e.g., None)\n",
    "                (x_data, y_batch), _ = data[:2]  # Discard unnecessary parts\n",
    "            else:\n",
    "                (x_data, y_batch) = data\n",
    "\n",
    "            # Further unpack x_data into images and temperature\n",
    "            x_batch, temp_batch = x_data\n",
    "\n",
    "            # Proceed with the rest of the train_step logic\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self.model(x_batch, training=True)\n",
    "                # Compute the loss value\n",
    "                loss = physics_informed_loss(y_batch, y_pred, temp_batch)\n",
    "\n",
    "            # Compute gradients\n",
    "            trainable_vars = self.model.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "            # Update weights\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "            # Update metrics\n",
    "            self.compiled_metrics.update_state(y_batch, y_pred)\n",
    "\n",
    "            # Return a dict mapping metric names to current value\n",
    "            return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error in train_step:\", e)\n",
    "            raise\n",
    "\n",
    "    def test_step(self, data):\n",
    "\n",
    "        try:\n",
    "            # Adjust unpacking to handle nested structure\n",
    "            if len(data) == 3:  # If there are extra elements (e.g., None)\n",
    "                (x_data, y_batch), _ = data[:2]  # Discard unnecessary parts\n",
    "            else:\n",
    "                (x_data, y_batch) = data\n",
    "\n",
    "            # Further unpack x_data into images and temperature\n",
    "            x_batch, temp_batch = x_data\n",
    "\n",
    "            # Compute predictions\n",
    "            y_pred = self.model(x_batch, training=False)\n",
    "\n",
    "            # Compute the loss value\n",
    "            loss = physics_informed_loss(y_batch, y_pred, temp_batch)\n",
    "\n",
    "            # Update metrics\n",
    "            self.compiled_metrics.update_state(y_batch, y_pred)\n",
    "\n",
    "            # Return a dict mapping metric names to current value\n",
    "            return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error in test_step:\", e)\n",
    "            raise\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Call method to process inputs during inference.\n",
    "        Handles both image and temperature inputs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            x_batch, temp_batch = inputs  # Unpack the tuple\n",
    "\n",
    "            return self.model(x_batch, training=False)\n",
    "        except Exception as e:\n",
    "            print(\"Error in call method:\", e)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Model Definitions & Initialization Tests\n",
    "\n",
    "##### Here, we will define the DL models to be used (e.g., VGG16 with fine-tuning, InceptionV3, CRNN with Attention) with necessary adjustments to accept temperature data where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. VGG16 with Fine-Tuning\n",
    "**Implementation details:**\n",
    "* Pre-trained VGG16 Model: Utilize the VGG16 model pre-trained on ImageNet.\n",
    "* Input Adjustments: Convert grayscale images to RGB by repeating the single channel three times.\n",
    "* Output Layer: Adjust the final dense layer to match the number of classes.\n",
    "* Temperature Handling: Temperature data is not fed into the model but provided to the loss function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Model:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape  # Shape: (height, width, channels)\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for images\n",
    "        inputs = Input(shape=self.input_shape, name='image_input')\n",
    "\n",
    "        # Convert grayscale to RGB by repeating channels\n",
    "        x = Concatenate(axis=-1)([inputs, inputs, inputs])  # Shape: (height, width, 3)\n",
    "\n",
    "        # Load pre-trained VGG16 model without the top layer and wiht pre-trained weights\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_tensor=x)\n",
    "\n",
    "        # Freeze base model layers for initial training (to keep pre-trained features during training)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom layers on top (more specifically, adds GlobalAveragePooling2D, a dense layer with 256 units and ReLU activation, and an output layer matching the number of classes with softmax activation)\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Construct the model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='VGG16Model')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        # Compilation will be handled in the training step using a custom training loop\n",
    "        pass  # No action needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 503ms/step\n",
      "VGG16 forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# Test VGG16Model\n",
    "\n",
    "# Initialize VGG16 Model using target_size\n",
    "vgg16_instance = VGG16Model(input_shape=target_size + (1,), num_classes=11)\n",
    "vgg16_base_model = vgg16_instance.model\n",
    "\n",
    "# Perform a forward pass with a dummy batch\n",
    "dummy_images = np.random.rand(2, *target_size, 1).astype(np.float32)  # Match target_size\n",
    "dummy_output = vgg16_base_model.predict(dummy_images)\n",
    "\n",
    "# Verify the output shape\n",
    "assert dummy_output.shape == (2, 11), f\"VGG16 output shape mismatch: {dummy_output.shape}\"\n",
    "print(\"VGG16 forward pass successful.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. InceptionV3\n",
    "**Implementation details:**\n",
    "* Pre-trained InceptionV3 Model: Utilize the InceptionV3 model pre-trained on ImageNet.\n",
    "* Input Adjustments: Convert grayscale images to RGB. Also resize images to the expected input size for InceptionV3 (e.g., 299x299).\n",
    "* Output Layer: Adjust the final dense layer to match the number of classes.\n",
    "* Temperature Handling: Temperature data is not fed into the model but provided to the loss function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3Model:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape  # Original image shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for images\n",
    "        inputs = Input(shape=self.input_shape, name='image_input')\n",
    "\n",
    "        # Resize images to 299x299 as expected by InceptionV3\n",
    "        x = Resizing(299, 299)(inputs)\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        x = Concatenate(axis=-1)([x, x, x])\n",
    "\n",
    "        # Load pre-trained InceptionV3 model without the top layer\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=x)\n",
    "\n",
    "        # Freeze base model layers for initial training\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom layers on top\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Construct the model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='InceptionV3Model')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        # Compilation will be handled during training with the custom loss\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "InceptionV3 forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# Test InceptionV3Model\n",
    "\n",
    "# Initialize InceptionV3 Model\n",
    "inception_instance = InceptionV3Model(input_shape=(*target_size, 1), num_classes=11)\n",
    "inception_base_model = inception_instance.model\n",
    "\n",
    "# Perform a forward pass with a dummy batch\n",
    "dummy_output = inception_base_model.predict(dummy_images)\n",
    "assert dummy_output.shape == (2, 11), f\"InceptionV3 output shape mismatch: {dummy_output.shape}\"\n",
    "print(\"InceptionV3 forward pass successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. CRNN with Attention Mechanism\n",
    "**Implementation details:**\n",
    "* Convolutional Layers: Extract spatial features from images.\n",
    "* Recurrent Layers (LSTM): Capture sequential dependencies in the extracted features.\n",
    "* Attention Mechanism: Enhance the model's focus on relevant features.\n",
    "* Input Adjustments: Use the grayscale images directly.\n",
    "* Output Layer: Adjust the final dense layer to match the number of classes.\n",
    "* Temperature Handling: Temperature data is not fed into the model but provided to the loss function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNNModel:\n",
    "    def __init__(self, input_shape, num_classes, lstm_units=64):\n",
    "        self.input_shape = input_shape  # Shape: (height, width, channels)\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for images\n",
    "        inputs = Input(shape=self.input_shape, name='image_input')\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        # Prepare data for LSTM\n",
    "        shape = K.int_shape(x)  # Use K.int_shape for dynamic dimensions\n",
    "        x = Reshape((shape[1] * shape[2], shape[3]))(x)  # Shape: (batch_size, timesteps, features)\n",
    "\n",
    "        # LSTM layer\n",
    "        x = LSTM(self.lstm_units, return_sequences=True)(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = Dense(1, activation='tanh')(x)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(self.lstm_units)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "        x = Multiply()([x, attention])\n",
    "        x = Lambda(lambda xin: K.sum(xin, axis=1))(x)\n",
    "\n",
    "        # Output layer\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Construct the model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='CRNNModel')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        # Compilation will be handled during training with the custom loss\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step\n",
      "CRNN forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# Test CRNNModel\n",
    "\n",
    "# Initialize CRNN Model\n",
    "crnn_instance = CRNNModel(input_shape=(*target_size, 1), num_classes=11)  # Use target_size for resized dimensions\n",
    "crnn_base_model = crnn_instance.model\n",
    "\n",
    "# Perform a forward pass with a dummy batch\n",
    "dummy_output = crnn_base_model.predict(dummy_images)\n",
    "assert dummy_output.shape == (2, 11), f\"CRNN output shape mismatch: {dummy_output.shape}\"\n",
    "print(\"CRNN forward pass successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/meso-home/vgarcia1/anaconda3/envs/impacts/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:639: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel training step successful. Results: [array(0.09090909, dtype=float32), {'accuracy': array(0.1875, dtype=float32)}]\n",
      "CustomModel testing step successful. Results: [array(0.09090909, dtype=float32), {'accuracy': array(0.1875, dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "# Verify CustomModel Training Logic\n",
    "# Use the CustomModel subclass with one architecture to ensure the train_step and test_step execute without errors\n",
    "\n",
    "# Wrap VGG16 model with CustomModel\n",
    "vgg16_model = CustomModel(vgg16_base_model)\n",
    "\n",
    "# Compile the model using the custom physics-informed loss\n",
    "vgg16_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=lambda y_true, y_pred: physics_informed_loss(y_true, y_pred, temp_batch),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fetch a batch\n",
    "(data_batch, temp_batch), label_batch = next(train_generator)\n",
    "\n",
    "# Perform one training step\n",
    "train_results = vgg16_model.train_on_batch(((data_batch, temp_batch), label_batch))\n",
    "print(\"CustomModel training step successful. Results:\", train_results)\n",
    "\n",
    "# Perform one testing step\n",
    "test_results = vgg16_model.test_on_batch(((data_batch, temp_batch), label_batch))\n",
    "print(\"CustomModel testing step successful. Results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Compliling the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class to compute F1-score TensorFlow does not provide a built-in F1 score metric\n",
    "class F1ScoreMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1_score\", **kwargs):\n",
    "        super(F1ScoreMetric, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "# Defining a class to compute RMSE since TensorFlow does not provide a built-in RMSE metric\n",
    "class RMSEMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"rmse\", **kwargs):\n",
    "        super(RMSEMetric, self).__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\", dtype=tf.float32)\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\", dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Ensure both inputs are float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        # Compute squared error\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "\n",
    "        # Update the total squared error and count\n",
    "        self.total.assign_add(tf.reduce_sum(squared_error))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Compute RMSE\n",
    "        mean_squared_error = self.total / (self.count + 1e-7)\n",
    "        return tf.sqrt(mean_squared_error)\n",
    "\n",
    "    def reset_states(self):\n",
    "        # Reset the total and count\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 model compiled successfully with F1-score and RMSE metrics.\n"
     ]
    }
   ],
   "source": [
    "# Wrap the VGG16 model in the CustomModel class\n",
    "vgg16_custom_model = CustomModel(vgg16_base_model)\n",
    "\n",
    "# Initialize the custom metrics\n",
    "f1_score_metric = F1ScoreMetric()\n",
    "rmse_metric = RMSEMetric()\n",
    "\n",
    "# Compile the VGG16 model with F1-score and RMSE metrics\n",
    "vgg16_custom_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        f1_score_metric,  # F1-score custom metric\n",
    "        rmse_metric,      # RMSE custom metric\n",
    "        'accuracy'        # Accuracy as a baseline metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"VGG16 model compiled successfully with F1-score and RMSE metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. InceptionV3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3 model compiled successfully with F1-score and RMSE metrics.\n"
     ]
    }
   ],
   "source": [
    "# Wrap the InceptionV3 model in the CustomModel class\n",
    "inception_custom_model = CustomModel(inception_base_model)\n",
    "\n",
    "# Compile the InceptionV3 model with F1-score and RMSE metrics\n",
    "inception_custom_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        f1_score_metric,  # F1-score custom metric\n",
    "        rmse_metric,      # RMSE custom metric\n",
    "        'accuracy'        # Accuracy as a baseline metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"InceptionV3 model compiled successfully with F1-score and RMSE metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. CRNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN model compiled successfully with F1-score and RMSE metrics.\n"
     ]
    }
   ],
   "source": [
    "# Wrap the CRNN model in the CustomModel class\n",
    "crnn_custom_model = CustomModel(crnn_base_model)\n",
    "\n",
    "# Compile the CRNN model with F1-score and RMSE metrics\n",
    "crnn_custom_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        f1_score_metric,  # F1-score custom metric\n",
    "        rmse_metric,      # RMSE custom metric\n",
    "        'accuracy'        # Accuracy as a baseline metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"CRNN model compiled successfully with F1-score and RMSE metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Training the Models (baseline before parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping to prevent overfitting (stops training when validation loss stops improving)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 4s/step - accuracy: 0.4288 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2744 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.5319 - f1_score: 0.0074 - precision: 0.5217 - recall: 0.0038 - rmse: 0.2647 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.5042 - f1_score: 0.0774 - precision: 1.0000 - recall: 0.0403 - rmse: 0.2513 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6489 - f1_score: 0.1322 - precision: 0.9565 - recall: 0.0723 - rmse: 0.2374 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7506 - f1_score: 0.2178 - precision: 1.0000 - recall: 0.1226 - rmse: 0.2260 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.7249 - f1_score: 0.3621 - precision: 0.9816 - recall: 0.2222 - rmse: 0.2215 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7293 - f1_score: 0.3788 - precision: 0.9885 - recall: 0.2359 - rmse: 0.2134 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.7501 - f1_score: 0.5187 - precision: 0.9707 - recall: 0.3542 - rmse: 0.2035 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7602 - f1_score: 0.5393 - precision: 0.9726 - recall: 0.3742 - rmse: 0.2023 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7531 - f1_score: 0.6310 - precision: 0.9480 - recall: 0.4734 - rmse: 0.1910 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.8029 - f1_score: 0.6594 - precision: 0.9922 - recall: 0.4951 - rmse: 0.1827 - loss: 0.0909 - val_loss: 0.0909\n",
      "VGG16 Training Time: 923.51 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the VGG16 Custom Model\n",
    "vgg16_history = vgg16_custom_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_steps=len(X_val) // batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "vgg16_training_time = time.time() - start_time\n",
    "print(f\"VGG16 Training Time: {vgg16_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. InceptionV3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.6669 - f1_score: 0.4526 - precision: 0.8360 - recall: 0.3133 - rmse: 0.2129 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.7708 - f1_score: 0.7028 - precision: 0.8682 - recall: 0.5909 - rmse: 0.1784 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.7849 - f1_score: 0.7712 - precision: 0.9112 - recall: 0.6738 - rmse: 0.1649 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8687 - f1_score: 0.8605 - precision: 0.9278 - recall: 0.8027 - rmse: 0.1309 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8353 - f1_score: 0.8353 - precision: 0.9389 - recall: 0.7551 - rmse: 0.1426 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8720 - f1_score: 0.8750 - precision: 0.9428 - recall: 0.8173 - rmse: 0.1301 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8888 - f1_score: 0.8823 - precision: 0.9272 - recall: 0.8416 - rmse: 0.1271 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9085 - f1_score: 0.8701 - precision: 0.9271 - recall: 0.8202 - rmse: 0.1267 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9089 - f1_score: 0.8965 - precision: 0.9524 - recall: 0.8471 - rmse: 0.1210 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9087 - f1_score: 0.9007 - precision: 0.9417 - recall: 0.8645 - rmse: 0.1205 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9419 - f1_score: 0.9258 - precision: 0.9793 - recall: 0.8786 - rmse: 0.1093 - loss: 0.0909 - val_loss: 0.0909\n",
      "InceptionV3 Training Time: 308.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the InceptionV3 Custom Model\n",
    "inception_history = inception_custom_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_steps=len(X_val) // batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "inception_training_time = time.time() - start_time\n",
    "print(f\"InceptionV3 Training Time: {inception_training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. CRNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.0731 - f1_score: 0.2139 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2692 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.0911 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.1788 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2853 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.1831 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2782 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.2240 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2745 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.1969 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2754 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.2042 - f1_score: 0.0178 - precision: 0.4406 - recall: 0.0091 - rmse: 0.2738 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.2926 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2702 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.3374 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2692 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.3353 - f1_score: 0.0075 - precision: 0.2391 - recall: 0.0038 - rmse: 0.2651 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.3039 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2658 - loss: 0.0909 - val_loss: 0.0909\n",
      "CRNN Training Time: 631.34 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the CRNN Custom Model\n",
    "crnn_history = crnn_custom_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_steps=len(X_val) // batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "crnn_training_time = time.time() - start_time\n",
    "print(f\"CRNN Training Time: {crnn_training_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log training time of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results saved to DL_baseline_training_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save training histories and training times\n",
    "\n",
    "# Convert histories to plain Python objects for JSON serialization\n",
    "def serialize_history(history):\n",
    "    return {key: [float(value) for value in values] for key, values in history.items()}\n",
    "\n",
    "# Save training histories and training times\n",
    "baseline_results = {\n",
    "    'vgg16': {\n",
    "        'history': vgg16_history.history,\n",
    "        'training_time': vgg16_training_time\n",
    "    },\n",
    "    'inception': {\n",
    "        'history': inception_history.history,\n",
    "        'training_time': inception_training_time\n",
    "    },\n",
    "    'crnn': {\n",
    "        'history': crnn_history.history,\n",
    "        'training_time': crnn_training_time\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_file = 'DL_baseline_training_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=4)\n",
    "\n",
    "print(f\"Training results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Training the Models (now with parameter tuning)\n",
    "##### Here, we chose a few hyperparameters to tune for each model:\n",
    "* VGG16: Learning rate, number of dense units\n",
    "* InceptionV3: Learning rate, number of dense units\n",
    "* CRNN: Learning rate, LSTM units\n",
    "\n",
    "#### Definitions:\n",
    "1. **Learning Rate:** determines the step size for updating the model's weights during optimization. A high learning rate can lead to rapid convergence but risks overshooting the optimal point, while a low learning rate ensures stability but may slow down convergence or get stuck in local minima. Proper tuning balances speed and accuracy.\n",
    "2. **Number of dense units:** number of units in dense (fully connected) layers defines the network's capacity to learn complex representations. More units increase the model's ability to capture nuanced patterns but risk overfitting and higher computational cost. Fewer units may lead to underfitting, where the model struggles to capture the data's complexity.\n",
    "3. **LSTM Units:** the number of LSTM (Long Short-Term Memory) units in recurrent layers determines the model's memory capacity for sequential dependencies. More units allow the model to retain and process longer-term patterns but increase the risk of overfitting and slower training. Fewer units may result in insufficient sequence learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generalized training function to run training for a specific model with given hyperparameters and save the results.\n",
    "\n",
    "def train_with_params(model, model_name, params, train_gen, val_gen, batch_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = params.get(\"learning_rate\", 1e-3)\n",
    "    dense_units = params.get(\"dense_units\", 256)\n",
    "    lstm_units = params.get(\"lstm_units\", 64)\n",
    "\n",
    "    # Adjust model architecture if needed\n",
    "    if model_name == \"VGG16\" or model_name == \"InceptionV3\":\n",
    "        # Locate the Dense layer using the model tree\n",
    "        dense_layer_found = False\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                layer.units = dense_units\n",
    "                dense_layer_found = True\n",
    "                break\n",
    "            elif hasattr(layer, 'layers'):  # For nested models\n",
    "                for nested_layer in layer.layers:\n",
    "                    if isinstance(nested_layer, tf.keras.layers.Dense):\n",
    "                        nested_layer.units = dense_units\n",
    "                        dense_layer_found = True\n",
    "                        break\n",
    "        if not dense_layer_found:\n",
    "            raise ValueError(f\"No Dense layer found in {model_name} for adjustment.\")\n",
    "        \n",
    "    elif model_name == \"CRNN\":\n",
    "        # Rebuild CRNN model\n",
    "        print(f\"Rebuilding CRNN model with lstm_units={lstm_units} and dense_units={dense_units}\")\n",
    "        rebuilt_model = CRNNModel(\n",
    "            input_shape=target_size + (1,),\n",
    "            num_classes=11,\n",
    "            lstm_units=lstm_units  # Update LSTM units\n",
    "        ).model\n",
    "        # Wrap the rebuilt model in CustomModel\n",
    "        model = CustomModel(rebuilt_model)\n",
    "\n",
    "    # Recompile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            F1ScoreMetric(),  # Custom F1-score class\n",
    "            RMSEMetric(),  # Custom RMSE class\n",
    "            \"accuracy\"  # Baseline accuracy\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=20,\n",
    "        steps_per_epoch=len(X_train) // batch_size,\n",
    "        validation_steps=len(X_val) // batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"history\": history.history,\n",
    "        \"training_time\": training_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning VGG16 with params: {'learning_rate': 0.001, 'dense_units': 128}\n",
      "VGG16 model layers: ['VGG16Model']\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/meso-home/vgarcia1/anaconda3/envs/impacts/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:639: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4s/step - accuracy: 0.5344 - f1_score: 0.0117 - precision: 0.8261 - recall: 0.0059 - rmse: 0.2623 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6298 - f1_score: 0.0679 - precision: 0.9565 - recall: 0.0353 - rmse: 0.2503 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6218 - f1_score: 0.1204 - precision: 0.8310 - recall: 0.0650 - rmse: 0.2422 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.7288 - f1_score: 0.2876 - precision: 0.9907 - recall: 0.1685 - rmse: 0.2241 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 4s/step - accuracy: 0.6938 - f1_score: 0.3697 - precision: 0.9750 - recall: 0.2293 - rmse: 0.2171 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - accuracy: 0.7568 - f1_score: 0.4784 - precision: 0.9333 - recall: 0.3221 - rmse: 0.2063 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.7064 - f1_score: 0.5208 - precision: 0.9039 - recall: 0.3663 - rmse: 0.2063 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7814 - f1_score: 0.6110 - precision: 0.9391 - recall: 0.4531 - rmse: 0.1898 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.8021 - f1_score: 0.6387 - precision: 0.9511 - recall: 0.4809 - rmse: 0.1890 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.8107 - f1_score: 0.6701 - precision: 0.9420 - recall: 0.5228 - rmse: 0.1811 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.8286 - f1_score: 0.6778 - precision: 0.9599 - recall: 0.5242 - rmse: 0.1789 - loss: 0.0909 - val_loss: 0.0909\n",
      "Tuning VGG16 with params: {'learning_rate': 0.0001, 'dense_units': 256}\n",
      "VGG16 model layers: ['VGG16Model']\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 4s/step - accuracy: 0.6631 - f1_score: 0.0072 - precision: 0.5652 - recall: 0.0036 - rmse: 0.2542 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.6896 - f1_score: 0.0104 - precision: 0.3913 - recall: 0.0053 - rmse: 0.2513 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6908 - f1_score: 0.0149 - precision: 0.6522 - recall: 0.0075 - rmse: 0.2511 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7130 - f1_score: 0.0257 - precision: 0.9565 - recall: 0.0131 - rmse: 0.2485 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.7090 - f1_score: 0.0352 - precision: 0.8261 - recall: 0.0180 - rmse: 0.2467 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7503 - f1_score: 0.0631 - precision: 0.9565 - recall: 0.0327 - rmse: 0.2446 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6595 - f1_score: 0.0639 - precision: 0.9565 - recall: 0.0331 - rmse: 0.2477 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7400 - f1_score: 0.0906 - precision: 0.9130 - recall: 0.0478 - rmse: 0.2406 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7524 - f1_score: 0.0999 - precision: 0.9565 - recall: 0.0528 - rmse: 0.2405 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7862 - f1_score: 0.0758 - precision: 0.9565 - recall: 0.0395 - rmse: 0.2402 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7022 - f1_score: 0.0977 - precision: 0.9565 - recall: 0.0518 - rmse: 0.2413 - loss: 0.0909 - val_loss: 0.0909\n",
      "Tuning VGG16 with params: {'learning_rate': 1e-05, 'dense_units': 512}\n",
      "VGG16 model layers: ['VGG16Model']\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 4s/step - accuracy: 0.7586 - f1_score: 0.0073 - precision: 0.5652 - recall: 0.0037 - rmse: 0.2506 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6940 - f1_score: 0.0369 - precision: 0.9130 - recall: 0.0188 - rmse: 0.2489 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7109 - f1_score: 0.0219 - precision: 0.8696 - recall: 0.0111 - rmse: 0.2509 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7153 - f1_score: 0.0256 - precision: 1.0000 - recall: 0.0131 - rmse: 0.2513 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7378 - f1_score: 0.0148 - precision: 0.7391 - recall: 0.0075 - rmse: 0.2498 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7449 - f1_score: 0.0440 - precision: 0.8696 - recall: 0.0226 - rmse: 0.2494 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7015 - f1_score: 0.0260 - precision: 0.7826 - recall: 0.0133 - rmse: 0.2516 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.6954 - f1_score: 0.0299 - precision: 0.8696 - recall: 0.0152 - rmse: 0.2490 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - accuracy: 0.7099 - f1_score: 0.0418 - precision: 0.9565 - recall: 0.0214 - rmse: 0.2491 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7673 - f1_score: 0.0791 - precision: 1.0000 - recall: 0.0413 - rmse: 0.2463 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.7512 - f1_score: 0.0227 - precision: 0.8696 - recall: 0.0115 - rmse: 0.2472 - loss: 0.0909 - val_loss: 0.0909\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune\n",
    "vgg16_params_list = [\n",
    "    {\"learning_rate\": 1e-3, \"dense_units\": 128},\n",
    "    {\"learning_rate\": 1e-4, \"dense_units\": 256},\n",
    "    {\"learning_rate\": 1e-5, \"dense_units\": 512}\n",
    "]\n",
    "\n",
    "# Train the VGG16 model for each parameter set\n",
    "vgg16_tuning_results = []\n",
    "for params in vgg16_params_list:\n",
    "    print(f\"Tuning VGG16 with params: {params}\")\n",
    "    result = train_with_params(\n",
    "        vgg16_custom_model, \"VGG16\", params, train_generator, val_generator, batch_size\n",
    "    )\n",
    "    vgg16_tuning_results.append({\"params\": params, \"result\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. InceptionV3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning InceptionV3 with params: {'learning_rate': 0.001, 'dense_units': 128}\n",
      "InceptionV3 model layers: ['InceptionV3Model']\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - accuracy: 0.7287 - f1_score: 0.6996 - precision: 0.8415 - recall: 0.5990 - rmse: 0.1850 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.7790 - f1_score: 0.7740 - precision: 0.8913 - recall: 0.6843 - rmse: 0.1679 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8393 - f1_score: 0.8209 - precision: 0.9179 - recall: 0.7429 - rmse: 0.1528 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8580 - f1_score: 0.8351 - precision: 0.8890 - recall: 0.7876 - rmse: 0.1459 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8789 - f1_score: 0.8722 - precision: 0.9294 - recall: 0.8218 - rmse: 0.1291 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9137 - f1_score: 0.9017 - precision: 0.9344 - recall: 0.8716 - rmse: 0.1214 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9246 - f1_score: 0.8983 - precision: 0.9363 - recall: 0.8634 - rmse: 0.1179 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8905 - f1_score: 0.8586 - precision: 0.9285 - recall: 0.7991 - rmse: 0.1308 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9174 - f1_score: 0.8947 - precision: 0.9428 - recall: 0.8515 - rmse: 0.1185 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9208 - f1_score: 0.9109 - precision: 0.9534 - recall: 0.8721 - rmse: 0.1102 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9272 - f1_score: 0.9112 - precision: 0.9543 - recall: 0.8720 - rmse: 0.1093 - loss: 0.0909 - val_loss: 0.0909\n",
      "Tuning InceptionV3 with params: {'learning_rate': 0.0001, 'dense_units': 256}\n",
      "InceptionV3 model layers: ['InceptionV3Model']\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.7893 - f1_score: 0.7397 - precision: 0.8656 - recall: 0.6475 - rmse: 0.1684 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8631 - f1_score: 0.7669 - precision: 0.8868 - recall: 0.6775 - rmse: 0.1626 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8801 - f1_score: 0.8522 - precision: 0.9304 - recall: 0.7868 - rmse: 0.1401 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8597 - f1_score: 0.8214 - precision: 0.9082 - recall: 0.7500 - rmse: 0.1425 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8670 - f1_score: 0.8270 - precision: 0.9016 - recall: 0.7642 - rmse: 0.1483 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9067 - f1_score: 0.8652 - precision: 0.9456 - recall: 0.7976 - rmse: 0.1330 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8748 - f1_score: 0.8544 - precision: 0.9199 - recall: 0.7977 - rmse: 0.1346 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8930 - f1_score: 0.8452 - precision: 0.9405 - recall: 0.7677 - rmse: 0.1383 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.9191 - f1_score: 0.9022 - precision: 0.9476 - recall: 0.8627 - rmse: 0.1177 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8943 - f1_score: 0.8525 - precision: 0.9513 - recall: 0.7729 - rmse: 0.1331 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8928 - f1_score: 0.8836 - precision: 0.9491 - recall: 0.8270 - rmse: 0.1296 - loss: 0.0909 - val_loss: 0.0909\n",
      "Tuning InceptionV3 with params: {'learning_rate': 1e-05, 'dense_units': 512}\n",
      "InceptionV3 model layers: ['InceptionV3Model']\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.8755 - f1_score: 0.8450 - precision: 0.9475 - recall: 0.7627 - rmse: 0.1421 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8702 - f1_score: 0.8394 - precision: 0.9252 - recall: 0.7687 - rmse: 0.1395 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8776 - f1_score: 0.8528 - precision: 0.9425 - recall: 0.7790 - rmse: 0.1386 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.9084 - f1_score: 0.8596 - precision: 0.9356 - recall: 0.7957 - rmse: 0.1350 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8914 - f1_score: 0.8570 - precision: 0.9371 - recall: 0.7899 - rmse: 0.1407 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8732 - f1_score: 0.8225 - precision: 0.9377 - recall: 0.7330 - rmse: 0.1446 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8823 - f1_score: 0.8528 - precision: 0.9210 - recall: 0.7949 - rmse: 0.1398 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.9092 - f1_score: 0.8590 - precision: 0.9471 - recall: 0.7863 - rmse: 0.1367 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8705 - f1_score: 0.8426 - precision: 0.9274 - recall: 0.7726 - rmse: 0.1429 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8840 - f1_score: 0.8299 - precision: 0.9475 - recall: 0.7388 - rmse: 0.1421 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8482 - f1_score: 0.8368 - precision: 0.9338 - recall: 0.7594 - rmse: 0.1434 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 12/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8801 - f1_score: 0.8751 - precision: 0.9344 - recall: 0.8245 - rmse: 0.1310 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 13/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8669 - f1_score: 0.8103 - precision: 0.9140 - recall: 0.7283 - rmse: 0.1467 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 14/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8607 - f1_score: 0.8133 - precision: 0.9141 - recall: 0.7330 - rmse: 0.1453 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 15/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8756 - f1_score: 0.8507 - precision: 0.9404 - recall: 0.7768 - rmse: 0.1383 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 16/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8595 - f1_score: 0.8320 - precision: 0.9305 - recall: 0.7532 - rmse: 0.1432 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 17/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.8826 - f1_score: 0.8615 - precision: 0.9518 - recall: 0.7880 - rmse: 0.1352 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 18/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.9164 - f1_score: 0.8937 - precision: 0.9671 - recall: 0.8316 - rmse: 0.1278 - loss: 0.0909 - val_loss: 0.0909\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune for InceptionV3\n",
    "inception_params_list = [\n",
    "    {\"learning_rate\": 1e-3, \"dense_units\": 128},\n",
    "    {\"learning_rate\": 1e-4, \"dense_units\": 256},\n",
    "    {\"learning_rate\": 1e-5, \"dense_units\": 512}\n",
    "]\n",
    "\n",
    "# Train the InceptionV3 model for each parameter set\n",
    "inception_tuning_results = []\n",
    "for params in inception_params_list:\n",
    "    print(f\"Tuning InceptionV3 with params: {params}\")\n",
    "    result = train_with_params(\n",
    "        inception_custom_model, \"InceptionV3\", params, train_generator, val_generator, batch_size\n",
    "    )\n",
    "    inception_tuning_results.append({\"params\": params, \"result\": result})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. CRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning CRNN with params: {'learning_rate': 0.001, 'dense_units': 256, 'lstm_units': 64}\n",
      "Rebuilding CRNN model with lstm_units=64 and dense_units=256\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.1070 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2876 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.0766 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.1469 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2857 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.1443 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2800 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.2095 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2735 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.2177 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2751 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.2469 - f1_score: 0.0099 - precision: 0.4783 - recall: 0.0050 - rmse: 0.2754 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.2444 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2707 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.4171 - f1_score: 0.0029 - precision: 0.0580 - recall: 0.0015 - rmse: 0.2653 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.3779 - f1_score: 0.0611 - precision: 0.7490 - recall: 0.0328 - rmse: 0.2623 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.4332 - f1_score: 0.2035 - precision: 0.8666 - recall: 0.1157 - rmse: 0.2548 - loss: 0.0909 - val_loss: 0.0909\n",
      "Tuning CRNN with params: {'learning_rate': 0.0001, 'dense_units': 512, 'lstm_units': 128}\n",
      "Rebuilding CRNN model with lstm_units=128 and dense_units=512\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.0818 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.0957 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2875 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.1119 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.0849 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.0682 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.0961 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2873 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.1355 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2872 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.1053 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2871 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.1312 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2868 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.2211 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2855 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.1718 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2805 - loss: 0.0909 - val_loss: 0.0909\n",
      "Tuning CRNN with params: {'learning_rate': 1e-05, 'dense_units': 128, 'lstm_units': 32}\n",
      "Rebuilding CRNN model with lstm_units=32 and dense_units=128\n",
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.0899 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2876 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - accuracy: 0.0894 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2875 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - accuracy: 0.1032 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.0703 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.1342 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2872 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.0896 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2876 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.0856 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2875 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.0934 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2875 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.0721 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2877 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.1186 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n",
      "Epoch 11/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - accuracy: 0.0899 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - rmse: 0.2874 - loss: 0.0909 - val_loss: 0.0909\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune for CRNN\n",
    "crnn_params_list = [\n",
    "    {\"learning_rate\": 1e-3, \"dense_units\": 256, \"lstm_units\": 64},\n",
    "    {\"learning_rate\": 1e-4, \"dense_units\": 512, \"lstm_units\": 128},\n",
    "    {\"learning_rate\": 1e-5, \"dense_units\": 128, \"lstm_units\": 32}\n",
    "]\n",
    "\n",
    "# Train the CRNN model for each parameter set\n",
    "crnn_tuning_results = []\n",
    "for params in crnn_params_list:\n",
    "    print(f\"Tuning CRNN with params: {params}\")\n",
    "    result = train_with_params(\n",
    "        None, \"CRNN\", params, train_generator, val_generator, batch_size\n",
    "    )\n",
    "    crnn_tuning_results.append({\"params\": params, \"result\": result})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving all tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tuning results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save all tuning results to a JSON file\n",
    "tuning_results = {\n",
    "    \"vgg16_tuning\": vgg16_tuning_results,\n",
    "    \"inception_tuning\": inception_tuning_results,\n",
    "    \"crnn_tuning\": crnn_tuning_results\n",
    "}\n",
    "\n",
    "with open(\"DL_tuning_training_results.json\", \"w\") as f:\n",
    "    json.dump(tuning_results, f)\n",
    "\n",
    "print(\"All tuning results saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "baseline_results = {\n",
    "    \"VGG16\": {\"history\": vgg16_history.history, \"training_time\": vgg16_training_time},\n",
    "    \"Inception\": {\"history\": inception_history.history, \"training_time\": inception_training_time},\n",
    "    \"CRNN\": {\"history\": crnn_history.history, \"training_time\": crnn_training_time},\n",
    "}\n",
    "\n",
    "# Load parameter tuning results\n",
    "tuning_results = {\n",
    "    \"VGG16\": vgg16_tuning_results,\n",
    "    \"Inception\": inception_tuning_results,\n",
    "    \"CRNN\": crnn_tuning_results,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics like accuracy, precision, recall, F1-score, RMSE, and training time.\n",
    "\n",
    "def evaluate_results(baseline_results, tuning_results):\n",
    "    print(\"\\n=== Baseline vs. Tuning Comparison ===\")\n",
    "    \n",
    "    models = [\"VGG16\", \"Inception\", \"CRNN\"]\n",
    "    metrics_to_compare = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"rmse\"]\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Baseline metrics\n",
    "        baseline = baseline_results[model_name]\n",
    "        baseline_metrics = {metric: max(baseline[\"history\"].get(metric, [0])) for metric in metrics_to_compare}\n",
    "        baseline_time = baseline[\"training_time\"]\n",
    "        print(f\"Baseline Metrics: {baseline_metrics}\")\n",
    "        print(f\"Baseline Training Time: {baseline_time:.2f} seconds\")\n",
    "        \n",
    "        # Best tuned metrics\n",
    "        best_tuned_result = max(tuning_results[model_name], key=lambda x: max(x[\"result\"][\"history\"].get(\"val_accuracy\", [0])))\n",
    "        best_params = best_tuned_result[\"params\"]\n",
    "        best_tuned_metrics = {metric: max(best_tuned_result[\"result\"][\"history\"].get(metric, [0])) for metric in metrics_to_compare}\n",
    "        best_tuned_time = best_tuned_result[\"result\"][\"training_time\"]\n",
    "        \n",
    "        print(f\"Tuned Metrics (Best): {best_tuned_metrics}\")\n",
    "        print(f\"Tuned Training Time: {best_tuned_time:.2f} seconds\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Software Delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
