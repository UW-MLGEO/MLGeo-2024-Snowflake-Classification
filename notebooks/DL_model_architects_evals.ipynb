{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Architecture Exploration and Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the `data_EDA_and_CML_benchmarking.ipynb` notebook for parts 1 and 2, which include the deep learning dataset preparation and CML benchmarking, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Architecture Exploration: Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall, the performances of the initial four deep learning models implemented in the `data_EDA_and_CML_benchmarking.ipynb` notebook, which included FCN, CNN, ResNet, and RNN, were poor. Among them, the CNN had the highest accuracy, exceeding 25%. While this value is still low, we will focus on implementing architectures that utilize CNNs, focusing on the three architectures listed below:  \n",
    "\n",
    "1. VGG16 with Fine-Tuning (a deep CNN)\n",
    "* *Why?* A VGG16 is a deep CNN with 16 layers that excels at deep feature extraction, effectively capturing complex visual features through small 3x3 convolutional filters. By using pre-trained weights on ImageNet and fine-tuning them on the `PHIPS_CrystalHabitAI_Dataset.nc` image dataset, VGG16 can adapt to our specific classification task, improving performance even with limited data, as the `PHIPS_CrystalHabitAI_Dataset.nc` image dataset is relatively small. The VGG16's depth and fine-tuning capabilities help overcome the low accuracy of initial models by learning more intricate patterns specific to our ice crystal images.\n",
    "\n",
    "2. InceptionV3 (a different variation of a deep CNN)\n",
    "* *Why?* This architecture excels at multi-scale feature learning, utilizing Inception modules to process multiple convolutional filter sizes in parallel, capturing visual information at different scales within the same layer. Despite its depth, InceptionV3 is computationally efficient due to techniques like factorized convolutions and dimension reductions, making it suitable for complex datasets without excessive computational cost. Its advanced architecture can extract richer and more diverse features than simpler models, potentially leading to significant improvements in classification accuracy on the `PHIPS_CrystalHabitAI_Dataset.nc` image dataset.\n",
    "\n",
    "3. Convolutional Recurrent Neural Network (CRNN) with Attention Mechanism (a hyrbid of CNN and RNN)\n",
    "* *Why?* CRNN integrates Convolutional Neural Networks for spatial feature extraction with Recurrent Neural Networks (like LSTM or GRU) to capture sequential or temporal dependencies in the data. Incorporating attention layers enables the model to focus on the most relevant parts of the input images, enhancing its ability to learn important features and improving classification results. Lastly, this architecture offers a novel solution that goes beyond standard models, potentially capturing complex patterns and relationships in our ice crystal images that previous models may have missed.\n",
    "\n",
    "#### By using these DL architectures, we will address the low performance of the initial DL models by leveraging deeper networks, advanced feature extraction techniques, and innovative combinations of neural network types tailored to our image classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.2 Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv2D, MaxPooling2D, \n",
    "                                     GlobalAveragePooling2D, Input, SimpleRNN, LSTM, TimeDistributed, \n",
    "                                     Bidirectional, Attention)\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.preprocessing.image import smart_resize\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Concatenate, Resizing, Reshape, Permute, Multiply, Activation, RepeatVector, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn for metrics\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                             f1_score, precision_score, recall_score, mean_squared_error, roc_curve, auc)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Loading and Preprocessing\n",
    "##### organized using a `DatasetLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the dataset using xarray\n",
    "        ds = xr.open_dataset(self.file_path)\n",
    "        images = ds['image_array'].values  # Shape: (samples, height, width)\n",
    "        labels = ds['label'].values        # Shape: (samples,)\n",
    "        temps = ds['temperature'].values   # Shape: (samples,)\n",
    "        return images, labels, temps\n",
    "\n",
    "    def preprocess_data(self, images, labels):\n",
    "        # Encode string labels into integers\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)\n",
    "        num_classes = len(np.unique(labels_encoded))\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        labels_one_hot = to_categorical(labels_encoded, num_classes)\n",
    "\n",
    "        # Expand dimensions of images for channels (grayscale images)\n",
    "        images_expanded = np.expand_dims(images, axis=-1)  # Shape: (samples, height, width, 1)\n",
    "\n",
    "        # Normalize images to [0, 1]\n",
    "        images_normalized = images_expanded / 255.0\n",
    "\n",
    "        return images_normalized, labels_one_hot, labels_encoded, num_classes, label_encoder\n",
    "\n",
    "    def split_data(self, images, labels_encoded, labels_one_hot, temps):\n",
    "        # First split: training set and temp set\n",
    "        X_train, X_temp, y_train_encoded, y_temp_encoded, y_train_one_hot, y_temp_one_hot, temp_train, temp_temp = train_test_split(\n",
    "            images, labels_encoded, labels_one_hot, temps, test_size=0.2, random_state=42, stratify=labels_encoded)\n",
    "\n",
    "        # Second split: validation set and test set\n",
    "        X_val, X_test, y_val_encoded, y_test_encoded, y_val_one_hot, y_test_one_hot, temp_val, temp_test = train_test_split(\n",
    "            X_temp, y_temp_encoded, y_temp_one_hot, temp_temp, test_size=0.5, random_state=42, stratify=y_temp_encoded)\n",
    "\n",
    "        return (X_train, y_train_encoded, y_train_one_hot, temp_train), \\\n",
    "               (X_val, y_val_encoded, y_val_one_hot, temp_val), \\\n",
    "               (X_test, y_test_encoded, y_test_one_hot, temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DatasetLoader and load the data\n",
    "data_loader = DatasetLoader('/Users/valeriagarcia/Desktop/ESS569_Snowflake_Classification/PHIPS_CrystalHabitAI_Dataset.nc')\n",
    "images, labels, temps = data_loader.load_data()\n",
    "images, labels_one_hot, labels_encoded, num_classes, label_encoder = data_loader.preprocess_data(images, labels)\n",
    "\n",
    "# Split data including temperatures\n",
    "(X_train, y_train_encoded, y_train_one_hot, temp_train), \\\n",
    "(X_val, y_val_encoded, y_val_one_hot, temp_val), \\\n",
    "(X_test, y_test_encoded, y_test_one_hot, temp_test) = data_loader.split_data(images, labels_encoded, labels_one_hot, temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (352, 1024, 1360, 1)\n",
      "y_train_one_hot shape: (352, 11)\n",
      "temp_train shape: (352,)\n",
      "Data type of temp_train: float64\n"
     ]
    }
   ],
   "source": [
    "# Check Shapes and Data Types\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train_one_hot shape:\", y_train_one_hot.shape)\n",
    "print(\"temp_train shape:\", temp_train.shape)\n",
    "print(\"Data type of temp_train:\", temp_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Data Augmentation\n",
    "##### Here, we create a data augmentation generator (`data_generator`) for the training data that applies random transformations—including rotations up to 20 degrees, horizontal and vertical shifts up to 10% of the image size, horizontal and vertical flips, zooms up to 10%—to enhance the diversity of the dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, labels_one_hot, temperatures, batch_size, augment=False):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0,\n",
    "        rotation_range=20 if augment else 0,\n",
    "        width_shift_range=0.1 if augment else 0,\n",
    "        height_shift_range=0.1 if augment else 0,\n",
    "        horizontal_flip=augment,\n",
    "        vertical_flip=augment,\n",
    "        zoom_range=0.1 if augment else 0\n",
    "    )\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels_one_hot = np.array(labels_one_hot)\n",
    "    temperatures = np.array(temperatures)\n",
    "    num_samples = images.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        if augment:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            x_batch = images[batch_indices]\n",
    "            y_batch = labels_one_hot[batch_indices]\n",
    "            temp_batch = temperatures[batch_indices]\n",
    "            \n",
    "            x_batch_augmented = np.empty_like(x_batch)\n",
    "            for i, img in enumerate(x_batch):\n",
    "                x_batch_augmented[i] = datagen.random_transform(img)\n",
    "            \n",
    "            # Ensure correct output structure\n",
    "            yield (x_batch_augmented, temp_batch), y_batch\n",
    "\n",
    "# Create data generators\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = data_generator(\n",
    "    X_train, y_train_one_hot, temp_train, batch_size, augment=True\n",
    ")\n",
    "val_generator = data_generator(\n",
    "    X_val, y_val_one_hot, temp_val, batch_size, augment=False\n",
    ")\n",
    "test_generator = data_generator(\n",
    "    X_test, y_test_one_hot, temp_test, batch_size, augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generator outputs verified successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the data generators produce batches with the expected shapes, types, and values\n",
    "\n",
    "# Fetch a batch from the train generator\n",
    "(data_batch, temp_batch), label_batch = next(train_generator)\n",
    "\n",
    "# Verify shapes\n",
    "assert data_batch.shape == (32, 1024, 1360, 1), f\"Data batch shape mismatch: {data_batch.shape}\"\n",
    "assert temp_batch.shape == (32,), f\"Temperature batch shape mismatch: {temp_batch.shape}\"\n",
    "assert label_batch.shape == (32, 11), f\"Label batch shape mismatch: {label_batch.shape}\"\n",
    "\n",
    "print(\"Data generator outputs verified successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Physics-Informed Loss Function with Probabilistic Class Likelihoods\n",
    "\n",
    "##### In the cloud microphysics community, it is well-understood from laboratory studies that different ice crystal habits have a tendency to grow within a specific range of temperatures and relative humidity conditions. An example of the different temperature regimes is provided in Varcie et al. 2024:\n",
    "* *polycrystalline growth layer* (growth of polycrstals) -  may occur when the ambient temperature is below -18˚C\n",
    "* *dendritic growth layer* (growth of dendrites) - may occur when temperature is warmer than or equal to -18˚C and less than or equal to -12˚C\n",
    "* *plate growth layer* (growth of plates) - may occur where temperature is warmer than -12˚C and less than -8˚C\n",
    "* *needle growth layer* (growth of needles) - may occur where temperatures are warmer than or equal to -8˚C and less than -3˚C\n",
    "\n",
    "##### As our dataset only contains temperature in the metadata, we will focus on leveraging temperature and the temperature regimes above to create a custom loss function. Note the above temperature ranges refer to temperature layers over which certain ice crystals *may* grow, assuming other conditions, such as high ice/water supersaturations, are met. Moreover, particles that growth at cooler temperatures may still be observed at warmer temperatures due to sedimentation of the particles. \n",
    "\n",
    "##### Objective: Incorporate temperature-dependent class probabilities into the loss function to guide the model based on physical principles while allowing for natural variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/d4xxsc2518zggw88z1b9s5rh0000gn/T/ipykernel_22494/2751809565.py:24: RuntimeWarning: Mean of empty slice\n",
      "  mean_temp = np.nanmean(temps_)\n",
      "/Users/valeriagarcia/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "#### Create a mapping from class labels to their corresponding temperatures ####\n",
    "\n",
    "# Get unique class labels\n",
    "unique_classes = np.unique(labels_encoded)\n",
    "\n",
    "# Initialize a dictionary to hold temperatures for each class\n",
    "class_temperatures = {class_idx: [] for class_idx in unique_classes}\n",
    "\n",
    "# Populate the dictionary\n",
    "for idx, class_idx in enumerate(labels_encoded):\n",
    "    temp = temps[idx]\n",
    "    class_temperatures[class_idx].append(temp)\n",
    "\n",
    "#### Calculate the mean and standard deviation for the temperatures in each class ####\n",
    "\n",
    "# Initialize the dictionary to hold temperature statistics for each class\n",
    "class_temperature_stats = {}\n",
    "\n",
    "# Compute mean and standard deviation for each class, ignoring NaNs\n",
    "for class_idx in unique_classes:\n",
    "    temps_ = np.array(class_temperatures[class_idx])\n",
    "    \n",
    "    # Compute mean and standard deviation while ignoring NaNs\n",
    "    mean_temp = np.nanmean(temps_)\n",
    "    std_temp = np.nanstd(temps_)\n",
    "    \n",
    "    # Handle case where all temps are NaN\n",
    "    if np.isnan(mean_temp) or np.isnan(std_temp):\n",
    "        class_temperature_stats[class_idx] = {'mean': np.nan, 'std': np.nan}\n",
    "    else:\n",
    "        class_temperature_stats[class_idx] = {'mean': mean_temp, 'std': std_temp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected num_classes: 11, Actual num_classes: 11\n",
      "Number of classes matches the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Confirm the number of unique classes\n",
    "actual_num_classes = len(np.unique(labels_encoded))\n",
    "print(f\"Expected num_classes: {num_classes}, Actual num_classes: {actual_num_classes}\")\n",
    "\n",
    "# Ensure consistency\n",
    "if actual_num_classes != num_classes:\n",
    "    raise ValueError(\"Mismatch between num_classes and the actual number of classes in the dataset!\")\n",
    "else:\n",
    "    print(\"Number of classes matches the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mappings (Integer to Original Label):\n",
      "Class 0: aggregate\n",
      "Class 1: bullet_rosette\n",
      "Class 2: capped_column\n",
      "Class 3: column\n",
      "Class 4: dendrite\n",
      "Class 5: graupel\n",
      "Class 6: needle\n",
      "Class 7: plate\n",
      "Class 8: polycrystal\n",
      "Class 9: side_plane\n",
      "Class 10: tiny\n",
      "\n",
      "Class Distribution in Dataset:\n",
      "Class 0: 40 samples\n",
      "Class 1: 40 samples\n",
      "Class 2: 40 samples\n",
      "Class 3: 40 samples\n",
      "Class 4: 40 samples\n",
      "Class 5: 40 samples\n",
      "Class 6: 40 samples\n",
      "Class 7: 40 samples\n",
      "Class 8: 40 samples\n",
      "Class 9: 40 samples\n",
      "Class 10: 40 samples\n"
     ]
    }
   ],
   "source": [
    "# Print the mapping of integer-encoded labels to original class labels\n",
    "print(\"Class Mappings (Integer to Original Label):\")\n",
    "for class_idx, class_label in enumerate(label_encoder.classes_):\n",
    "    print(f\"Class {class_idx}: {class_label}\")\n",
    "\n",
    "# Check the number of samples per class in the dataset\n",
    "print(\"\\nClass Distribution in Dataset:\")\n",
    "unique_classes, class_counts = np.unique(labels_encoded, return_counts=True)\n",
    "for class_idx, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Class {class_idx}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Mean Temp = -11.28, Std Temp = 4.97\n",
      "Class 1: Temperature stats are missing (mean/std are NaN).\n",
      "Class 2: Mean Temp = -11.40, Std Temp = 2.89\n",
      "Class 3: Mean Temp = -13.08, Std Temp = 4.77\n",
      "Class 4: Mean Temp = -14.17, Std Temp = 2.99\n",
      "Class 5: Mean Temp = -11.17, Std Temp = 4.74\n",
      "Class 6: Mean Temp = -3.14, Std Temp = 1.18\n",
      "Class 7: Mean Temp = -9.33, Std Temp = 4.09\n",
      "Class 8: Mean Temp = -13.55, Std Temp = 4.28\n",
      "Class 9: Mean Temp = -9.21, Std Temp = 4.76\n",
      "Class 10: Mean Temp = -8.36, Std Temp = 3.61\n",
      "Temperature values for Class 0: [-15.21, -15.28, -15.25, -15.23, -15.23, -15.29, -15.34, -15.3, -15.28, -15.11, -15.01, -14.8, -12.16, -15.15, -15.0, -14.85, nan, -15.25, -17.89, -16.53, -16.33, -16.61, -7.67, -5.09, -4.64, -4.32, nan, nan, nan, -5.64, -5.77, -5.69, -5.23, -5.09, -4.9, -4.82, -4.89, -4.72, -4.79, -10.6]\n"
     ]
    }
   ],
   "source": [
    "# Verify class temperature statistics\n",
    "for class_idx, stats in class_temperature_stats.items():\n",
    "    if np.isnan(stats['mean']) or np.isnan(stats['std']):\n",
    "        print(f\"Class {class_idx}: Temperature stats are missing (mean/std are NaN).\")\n",
    "    else:\n",
    "        print(f\"Class {class_idx}: Mean Temp = {stats['mean']:.2f}, Std Temp = {stats['std']:.2f}\")\n",
    "\n",
    "# Spot-check the temperature values for a specific class\n",
    "class_to_check = 0  # Replace with a class index to check\n",
    "print(f\"Temperature values for Class {class_to_check}: {class_temperatures[class_to_check]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN temperatures: 111/440 (25.23%)\n",
      "Class 0 has 4 NaN temperature(s).\n",
      "Class 1 has 40 NaN temperature(s).\n",
      "Class 2 has 6 NaN temperature(s).\n",
      "Class 3 has 12 NaN temperature(s).\n",
      "Class 5 has 1 NaN temperature(s).\n",
      "Class 6 has 18 NaN temperature(s).\n",
      "Class 7 has 6 NaN temperature(s).\n",
      "Class 9 has 17 NaN temperature(s).\n",
      "Class 10 has 7 NaN temperature(s).\n"
     ]
    }
   ],
   "source": [
    "# Check the percentage of NaN temperatures\n",
    "nan_count = np.isnan(temps).sum()\n",
    "total_samples = len(temps)\n",
    "print(f\"Number of NaN temperatures: {nan_count}/{total_samples} ({(nan_count/total_samples)*100:.2f}%)\")\n",
    "\n",
    "# Investigate classes with NaN temperatures\n",
    "for class_idx, temps_ in class_temperatures.items():\n",
    "    nan_temps = [temp for temp in temps_ if np.isnan(temp)]\n",
    "    if nan_temps:\n",
    "        print(f\"Class {class_idx} has {len(nan_temps)} NaN temperature(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The temperature statistics are all NaN for the bullet rosette group (Class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_expected_probs() function calculates the expected class probabilities for each sample in a batch based on its temperature, using Gaussian distributions derived from the class temperature statistics.\n",
    "# This function will be called within the physics-informed loss function to obtain the expected probabilities based on temperature, which are then used to compute the physics term (e.g., KL divergence).\n",
    "# Output: Normalized probabilities for each sample in a batch.\n",
    "\n",
    "def get_expected_probs(temperature_batch, num_classes):\n",
    "    # Initialize expected probabilities array\n",
    "    expected_probs = np.zeros((len(temperature_batch), num_classes), dtype=np.float32)\n",
    "    \n",
    "    for i, temp in enumerate(temperature_batch):\n",
    "        total_prob = 0.0\n",
    "        probs = np.zeros(num_classes, dtype=np.float32)\n",
    "        \n",
    "        # Handle NaN temperatures by assigning uniform probabilities\n",
    "        if np.isnan(temp):\n",
    "            # Assign uniform probability if temperature is NaN\n",
    "            probs[:] = 1.0 / num_classes\n",
    "        else:\n",
    "            for class_idx in range(num_classes):\n",
    "                mean = class_temperature_stats[class_idx]['mean']\n",
    "                std = class_temperature_stats[class_idx]['std']\n",
    "                \n",
    "                # Handle classes with NaN mean or std by assigning uniform probability\n",
    "                if np.isnan(mean) or np.isnan(std):\n",
    "                    prob = 1.0 / num_classes\n",
    "                else:\n",
    "                    # Gaussian probability density function\n",
    "                    prob = np.exp(-0.5 * ((temp - mean) / std) ** 2) / (std * np.sqrt(2 * np.pi))\n",
    "                probs[class_idx] = prob\n",
    "                total_prob += prob\n",
    "            \n",
    "            # Normalize probabilities to sum to 1\n",
    "            if total_prob > 0:\n",
    "                probs /= total_prob\n",
    "            else:\n",
    "                # If total_prob is zero (unlikely), assign uniform probabilities\n",
    "                probs[:] = 1.0 / num_classes\n",
    "        \n",
    "        expected_probs[i] = probs\n",
    "    \n",
    "    return expected_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the categorical cross-entropy loss with a physics-informed KL divergence term.\n",
    "# This physics-informed loss function will be modified to exclude classes with missing temperature data (e.g., Class 1) from the physics term.\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "def physics_informed_loss(y_true, y_pred, temperature):\n",
    "    print(\"y_true shape:\", y_true.shape)\n",
    "    print(\"y_pred shape:\", y_pred.shape)\n",
    "    print(\"temperature shape:\", temperature.shape)\n",
    "\n",
    "    # Standard categorical cross-entropy loss\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    loss = cce(y_true, y_pred)\n",
    "\n",
    "    # Identify samples not belonging to classes with missing temperature stats\n",
    "    class_indices = tf.argmax(y_true, axis=1)\n",
    "    valid_class_mask = tf.constant([\n",
    "        not np.isnan(class_temperature_stats[i]['mean']) for i in range(num_classes)\n",
    "    ], dtype=tf.bool)\n",
    "    sample_mask = tf.gather(valid_class_mask, class_indices)\n",
    "    sample_mask = tf.cast(sample_mask, tf.float32)\n",
    "\n",
    "    # Compute expected probabilities based on temperature\n",
    "    expected_probs = tf.numpy_function(\n",
    "        func=get_expected_probs,\n",
    "        inp=[temperature, num_classes],\n",
    "        Tout=tf.float32\n",
    "    )\n",
    "\n",
    "    print(\"Expected probabilities shape:\", expected_probs.shape)\n",
    "\n",
    "    # Compute the physics term (KL divergence)\n",
    "    kl_divergence = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    physics_term = kl_divergence(expected_probs, y_pred)\n",
    "\n",
    "    print(\"Physics term shape before masking:\", physics_term.shape)\n",
    "\n",
    "    # Apply the mask to exclude invalid samples\n",
    "    physics_term = physics_term * sample_mask\n",
    "\n",
    "    # Compute mean physics term over valid samples\n",
    "    total_valid_samples = tf.reduce_sum(sample_mask) + 1e-7  # Avoid division by zero\n",
    "    physics_term = tf.reduce_sum(physics_term) / total_valid_samples\n",
    "\n",
    "    # Total loss with weighting factor\n",
    "    lambda_weight = 0.1  # Adjust as needed\n",
    "    total_loss = loss + lambda_weight * physics_term\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the loss function with dummy data to ensure it works without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:09:48.467828: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-12-02 15:09:48.468834: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-02 15:09:48.468850: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-02 15:09:48.469503: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-02 15:09:48.470492: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.4944966\n"
     ]
    }
   ],
   "source": [
    "# Set your actual number of classes\n",
    "num_classes = 11\n",
    "\n",
    "# Dummy data for testing\n",
    "y_true_test = tf.one_hot([0, 1], depth=num_classes)\n",
    "y_pred_test = tf.constant([[0.1]*num_classes, [0.1]*num_classes], dtype=tf.float32)\n",
    "temp_test = tf.constant([0.0, -5.0], dtype=tf.float32)\n",
    "\n",
    "# Call the loss function\n",
    "test_loss = physics_informed_loss(y_true_test, y_pred_test, temp_test)\n",
    "print(\"Test loss:\", test_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The loss value obtained is reasonable and aligns with the expected calculations given the dummy data. The loss value is relatively high (~2.49) because the predicted probabilities are uniform and do not match the true labels. We have set the predicted probabilities to be uniform across all classes, so predicting the same probability for all classes (especially when incorrect) results in a higher loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Loss Value: 2.024979\n"
     ]
    }
   ],
   "source": [
    "# Verifying Temperature-Based Loss Function one more time to ensure the function works correctly with model outputs and temperature inputs, using a small batch of dummy data to simulate training and testing scenarios\n",
    "\n",
    "# Dummy data for verification\n",
    "dummy_images = np.random.rand(2, 1024, 1360, 1).astype(np.float32)  # 2 grayscale images\n",
    "dummy_labels = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])  # One-hot labels\n",
    "dummy_temps = np.array([0.0, -5.0], dtype=np.float32)  # 2 temperature values\n",
    "\n",
    "# Simulate model predictions (random probabilities)\n",
    "dummy_predictions = np.random.rand(2, 11).astype(np.float32)\n",
    "dummy_predictions /= np.sum(dummy_predictions, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "\n",
    "# Compute the loss\n",
    "dummy_loss = physics_informed_loss(\n",
    "    tf.constant(dummy_labels, dtype=tf.float32),\n",
    "    tf.constant(dummy_predictions, dtype=tf.float32),\n",
    "    tf.constant(dummy_temps, dtype=tf.float32)\n",
    ")\n",
    "\n",
    "print(\"Dummy Loss Value:\", dummy_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = physics_informed_loss(\n",
    "    tf.constant(label_batch),\n",
    "    tf.constant(np.ones_like(label_batch)),  # Dummy prediction\n",
    "    tf.constant(temp_batch)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lastly, to handle the passing of temperature data to the loss function, we'll use a subclassed model with a custom `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def train_step(self, data):\n",
    "        print(\"Incoming data structure in train_step:\", type(data), len(data))  # Debugging\n",
    "        print(\"Data content in train_step:\", data)  # Debugging\n",
    "\n",
    "        try:\n",
    "            # Adjust unpacking to handle nested structure\n",
    "            if len(data) == 3:  # If there are extra elements (e.g., None)\n",
    "                (x_data, y_batch), _ = data[:2]  # Discard unnecessary parts\n",
    "            else:\n",
    "                (x_data, y_batch) = data\n",
    "\n",
    "            # Further unpack x_data into images and temperature\n",
    "            x_batch, temp_batch = x_data\n",
    "\n",
    "            # Debugging: Check shapes and data types\n",
    "            print(\"x_batch shape:\", x_batch.shape)\n",
    "            print(\"temp_batch shape:\", temp_batch.shape)\n",
    "            print(\"y_batch shape:\", y_batch.shape)\n",
    "\n",
    "            # Proceed with the rest of the train_step logic\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self.model(x_batch, training=True)\n",
    "                # Compute the loss value\n",
    "                loss = physics_informed_loss(y_batch, y_pred, temp_batch)\n",
    "\n",
    "            # Compute gradients\n",
    "            trainable_vars = self.model.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "            # Update weights\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "            # Update metrics\n",
    "            self.compiled_metrics.update_state(y_batch, y_pred)\n",
    "\n",
    "            # Return a dict mapping metric names to current value\n",
    "            return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error in train_step:\", e)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Model Definitions\n",
    "\n",
    "##### Here, we will define the DL models to be used (e.g., VGG16 with fine-tuning, InceptionV3, CRNN with Attention) with necessary adjustments to accept temperature data where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. VGG16 with Fine-Tuning\n",
    "**Implementation details:**\n",
    "* Pre-trained VGG16 Model: Utilize the VGG16 model pre-trained on ImageNet.\n",
    "* Input Adjustments: Convert grayscale images to RGB by repeating the single channel three times.\n",
    "* Output Layer: Adjust the final dense layer to match the number of classes.\n",
    "* Temperature Handling: Temperature data is not fed into the model but provided to the loss function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Model:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape  # Shape: (height, width, channels)\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for images\n",
    "        inputs = Input(shape=self.input_shape, name='image_input')\n",
    "\n",
    "        # Convert grayscale to RGB by repeating channels\n",
    "        x = Concatenate(axis=-1)([inputs, inputs, inputs])  # Shape: (height, width, 3)\n",
    "\n",
    "        # Load pre-trained VGG16 model without the top layer and wiht pre-trained weights\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_tensor=x)\n",
    "\n",
    "        # Freeze base model layers for initial training (to keep pre-trained features during training)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom layers on top (more specifically, adds GlobalAveragePooling2D, a dense layer with 256 units and ReLU activation, and an output layer matching the number of classes with softmax activation)\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Construct the model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='VGG16Model')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        # Compilation will be handled in the training step using a custom training loop\n",
    "        pass  # No action needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:09:50.393766: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "VGG16 forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# Test VGG16Model\n",
    "\n",
    "# Initialize VGG16 Model\n",
    "vgg16_instance = VGG16Model(input_shape=(1024, 1360, 1), num_classes=11)\n",
    "vgg16_base_model = vgg16_instance.model\n",
    "\n",
    "# Perform a forward pass with a dummy batch\n",
    "dummy_output = vgg16_base_model.predict(dummy_images)\n",
    "assert dummy_output.shape == (2, 11), f\"VGG16 output shape mismatch: {dummy_output.shape}\"\n",
    "print(\"VGG16 forward pass successful.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. InceptionV3\n",
    "**Implementation details:**\n",
    "* Pre-trained InceptionV3 Model: Utilize the InceptionV3 model pre-trained on ImageNet.\n",
    "* Input Adjustments: Convert grayscale images to RGB. Also resize images to the expected input size for InceptionV3 (e.g., 299x299).\n",
    "* Output Layer: Adjust the final dense layer to match the number of classes.\n",
    "* Temperature Handling: Temperature data is not fed into the model but provided to the loss function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3Model:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape  # Original image shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for images\n",
    "        inputs = Input(shape=self.input_shape, name='image_input')\n",
    "\n",
    "        # Resize images to 299x299 as expected by InceptionV3\n",
    "        x = Resizing(299, 299)(inputs)\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        x = Concatenate(axis=-1)([x, x, x])  # Shape: (299, 299, 3)\n",
    "\n",
    "        # Load pre-trained InceptionV3 model without the top layer\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=x)\n",
    "\n",
    "        # Freeze base model layers for initial training\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom layers on top\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Construct the model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='InceptionV3Model')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        # Compilation will be handled during training with the custom loss\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "InceptionV3 forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# Test InceptionV3Model\n",
    "\n",
    "# Initialize InceptionV3 Model\n",
    "inception_instance = InceptionV3Model(input_shape=(1024, 1360, 1), num_classes=11)\n",
    "inception_base_model = inception_instance.model\n",
    "\n",
    "# Perform a forward pass with a dummy batch\n",
    "dummy_output = inception_base_model.predict(dummy_images)\n",
    "assert dummy_output.shape == (2, 11), f\"InceptionV3 output shape mismatch: {dummy_output.shape}\"\n",
    "print(\"InceptionV3 forward pass successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. CRNN with Attention Mechanism\n",
    "**Implementation details:**\n",
    "* Convolutional Layers: Extract spatial features from images.\n",
    "* Recurrent Layers (LSTM): Capture sequential dependencies in the extracted features.\n",
    "* Attention Mechanism: Enhance the model's focus on relevant features.\n",
    "* Input Adjustments: Use the grayscale images directly.\n",
    "* Output Layer: Adjust the final dense layer to match the number of classes.\n",
    "* Temperature Handling: Temperature data is not fed into the model but provided to the loss function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNNModel:\n",
    "    def __init__(self, input_shape, num_classes, lstm_units=64):\n",
    "        self.input_shape = input_shape  # Shape: (height, width, channels)\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input layer for images\n",
    "        inputs = Input(shape=self.input_shape, name='image_input')\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        # Prepare data for LSTM\n",
    "        shape = x.shape\n",
    "        x = Reshape((shape[1] * shape[2], shape[3]))(x)  # Shape: (batch_size, timesteps, features)\n",
    "\n",
    "        # LSTM layer\n",
    "        x = LSTM(self.lstm_units, return_sequences=True)(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = Dense(1, activation='tanh')(x)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(self.lstm_units)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "        x = Multiply()([x, attention])\n",
    "        x = Lambda(lambda xin: K.sum(xin, axis=1))(x)\n",
    "\n",
    "        # Output layer\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # Construct the model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='CRNNModel')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        # Compilation will be handled during training with the custom loss\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "CRNN forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# Test CRNNModel\n",
    "\n",
    "# Initialize CRNN Model\n",
    "crnn_instance = CRNNModel(input_shape=(1024, 1360, 1), num_classes=11)\n",
    "crnn_base_model = crnn_instance.model\n",
    "\n",
    "# Perform a forward pass with a dummy batch\n",
    "dummy_output = crnn_base_model.predict(dummy_images)\n",
    "assert dummy_output.shape == (2, 11), f\"CRNN output shape mismatch: {dummy_output.shape}\"\n",
    "print(\"CRNN forward pass successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming data structure in train_step: <class 'tuple'> 3\n",
      "Data content in train_step: (((<tf.Tensor 'data:0' shape=(32, 1024, 1360, 1) dtype=float64>, <tf.Tensor 'data_1:0' shape=(32,) dtype=float64>), <tf.Tensor 'data_2:0' shape=(32, 11) dtype=float64>), None, None)\n",
      "x_batch shape: (32, 1024, 1360, 1)\n",
      "temp_batch shape: (32,)\n",
      "y_batch shape: (32, 11)\n",
      "Error in train_step: Cannot take the length of shape with unknown rank.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take the length of shape with unknown rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m (data_batch, temp_batch), label_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_generator)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Perform one training step\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mvgg16_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomModel training step successful. Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn[60], line 29\u001b[0m, in \u001b[0;36mCustomModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_batch, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Compute the loss value\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m physics_informed_loss(y_batch, y_pred, temp_batch)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     32\u001b[0m trainable_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables\n",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m, in \u001b[0;36mphysics_informed_loss\u001b[0;34m(y_true, y_pred, temperature)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute the physics term (KL divergence)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mKLDivergence(reduction\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mReduction\u001b[38;5;241m.\u001b[39mNONE)\n\u001b[0;32m---> 27\u001b[0m physics_term \u001b[38;5;241m=\u001b[39m \u001b[43mkl_divergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Apply the mask to exclude invalid samples\u001b[39;00m\n\u001b[1;32m     30\u001b[0m physics_term \u001b[38;5;241m=\u001b[39m physics_term \u001b[38;5;241m*\u001b[39m sample_mask\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/losses/loss.py:67\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     60\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(y_true, y_pred)\n\u001b[1;32m     68\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_keras_mask(losses)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/losses/losses.py:28\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[0;32m---> 28\u001b[0m     y_true_y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqueeze_or_expand_to_same_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure_up_to(y_true, \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m], y_true_y_pred)\n\u001b[1;32m     32\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure_up_to(y_pred, \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], y_true_y_pred)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/tree/tree_api.py:174\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.tree.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructures):\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Maps `func` through given structures.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m        A new structure with the same layout as the given ones.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/tree/optree_impl.py:98\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     97\u001b[0m     assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnone_is_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/optree/ops.py:752\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    750\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    751\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/losses/loss.py:121\u001b[0m, in \u001b[0;36msqueeze_or_expand_to_same_rank\u001b[0;34m(x1, x2, expand_rank_1)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msqueeze_or_expand_to_same_rank\u001b[39m(x1, x2, expand_rank_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Squeeze/expand last dim if ranks differ from expected by exactly 1.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     x1_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     x2_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x1_rank \u001b[38;5;241m==\u001b[39m x2_rank:\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take the length of shape with unknown rank."
     ]
    }
   ],
   "source": [
    "# Verify CustomModel Training Logic\n",
    "# Use the CustomModel subclass with one architecture to ensure the train_step and test_step execute without errors\n",
    "\n",
    "# Wrap VGG16 model with CustomModel\n",
    "vgg16_model = CustomModel(vgg16_base_model)\n",
    "vgg16_model.compile(optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "\n",
    "# Fetch a batch\n",
    "(data_batch, temp_batch), label_batch = next(train_generator)\n",
    "\n",
    "# Perform one training step\n",
    "train_results = vgg16_model.train_on_batch(((data_batch, temp_batch), label_batch))\n",
    "print(\"CustomModel training step successful. Results:\", train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming data structure in train_step: <class 'tuple'> 3\n",
      "Data content in train_step: (((<tf.Tensor 'data:0' shape=(32, 1024, 1360, 1) dtype=float64>, <tf.Tensor 'data_1:0' shape=(32,) dtype=float64>), <tf.Tensor 'data_2:0' shape=(32, 11) dtype=float64>), None, None)\n",
      "Error unpacking data in train_step: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform one training step\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mvgg16_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomModel training step successful. Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Perform one testing step\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowflake_classific/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn[53], line 12\u001b[0m, in \u001b[0;36mCustomModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData content in train_step:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data)  \u001b[38;5;66;03m# Debugging\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Unpack the data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     (x_batch, temp_batch), y_batch \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Proceed with the rest of the train_step logic\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Perform one testing step\n",
    "test_results = vgg16_model.test_on_batch(((data_batch, temp_batch), label_batch))\n",
    "print(\"CustomModel testing step successful. Results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Compliling and Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
